"""Configuration for network generation.

This module defines attribute weights for similarity calculations,
degree correction multipliers, edge type rules, and influence factors.

NetworkConfig can be generated by LLM from a population spec or
loaded from YAML for manual customization.
"""

from pathlib import Path
from typing import Any, Literal

import yaml
from pydantic import BaseModel, Field


class AttributeWeightConfig(BaseModel):
    """Configuration for how an attribute contributes to similarity.

    Attributes:
        weight: Base weight for this attribute (higher = more important)
        match_type: How to compute match score:
            - "exact": 1 if exact match, 0 otherwise
            - "numeric_range": 1 - |A - B| / range (normalized difference)
            - "within_n": 1 if within n levels, 0 otherwise
        range_value: For numeric_range, the normalization range; for within_n, the allowed difference
        ordinal_levels: For within_n match type, maps option values to ordinal integers.
            If None, falls back to config-level ordinal_levels.
    """

    weight: float
    match_type: str = "exact"
    range_value: float | None = None
    ordinal_levels: dict[str, int] | None = None


class DegreeMultiplierConfig(BaseModel):
    """Configuration for degree correction multipliers.

    Certain agents are more connected based on their attributes.
    Multipliers stack multiplicatively.
    """

    attribute: str
    condition: Any  # Value to match (or callable for complex conditions)
    multiplier: float
    rationale: str


class EdgeTypeRule(BaseModel):
    """Rule for inferring edge types between connected agents.

    Rules are evaluated in priority order (highest first).
    The condition is a simple expression referencing agent attributes
    with `a_` and `b_` prefixes (e.g., "a_employer == b_employer").

    Attributes:
        name: Edge type label (e.g., "colleague", "neighbor")
        condition: Expression evaluated against agent pair attributes.
            Available variables: a_{attr} and b_{attr} for each agent attribute.
            Supports ==, !=, and, or, not, in.
        priority: Higher priority rules are checked first. Default 0.
        description: Human-readable explanation of when this edge type applies.
    """

    name: str
    condition: str
    priority: int = 0
    description: str = ""


class InfluenceFactorConfig(BaseModel):
    """Configuration for how an attribute affects influence weight between agents.

    Used by _compute_influence_weights() to determine asymmetric influence
    (e.g., senior people influence junior people more).

    Attributes:
        attribute: Agent attribute name to evaluate.
        type: How to interpret the attribute:
            - "ordinal": Uses levels mapping to compute ratio (higher influences lower)
            - "boolean": Presence/absence adds weight bonus
            - "numeric": Raw numeric ratio (dampened)
        levels: For ordinal type, maps attribute values to numeric rank.
            Higher rank = more influence.
        weight: How much this factor contributes. Default 0.2.
        description: Human-readable explanation.
    """

    attribute: str
    type: Literal["ordinal", "boolean", "numeric"]
    levels: dict[str, int] | None = None
    weight: float = 0.2
    description: str = ""


class NetworkConfig(BaseModel):
    """Complete configuration for network generation.

    Can be generated by LLM from a population spec, loaded from YAML,
    or constructed manually. Empty defaults produce a flat network.

    Attributes:
        avg_degree: Target average degree (connections per agent)
        rewire_prob: Watts-Strogatz rewiring probability
        similarity_store_threshold: Minimum similarity retained in sparse matrix
        similarity_threshold: Sigmoid threshold for edge probability
        similarity_steepness: Sigmoid steepness for edge probability
        candidate_mode: Similarity candidate strategy.
            - "exact": all-pairs (highest fidelity, slowest)
            - "blocked": block-based candidate pruning (near-equivalent, much faster)
        candidate_pool_multiplier: Candidate pool size per node as a multiple of avg_degree
        min_candidate_pool: Lower bound for candidate pool size per node in blocked mode
        blocking_attributes: Attributes used for blocking. Auto-selected if empty.
        similarity_workers: Worker processes for similarity stage (1 = serial)
        similarity_chunk_size: Row chunk size per worker task
        checkpoint_every_rows: Save similarity checkpoint every N rows
        triadic_closure_prob: Probability of closing open triads (A-B, B-C -> A-C).
            Higher values create more realistic clustering. Default 0.4.
        target_clustering: Target clustering coefficient (0.3-0.5 is realistic).
            Triadic closure runs until this is reached or max iterations hit.
        attribute_weights: Weights for similarity calculation
        degree_multipliers: Multipliers for degree correction
        edge_type_rules: Rules for inferring edge types (evaluated by priority)
        influence_factors: Factors for computing asymmetric influence weights
        default_edge_type: Fallback edge type when no rule matches
        ordinal_levels: Global ordinal level mappings (keyed by attribute name).
            Used by within_n match type when AttributeWeightConfig.ordinal_levels is None.
        generated_from: Description of what population this config was generated for
        generation_rationale: LLM's reasoning for the chosen configuration
        seed: Random seed for reproducibility
    """

    avg_degree: float = 20.0
    rewire_prob: float = 0.05
    similarity_store_threshold: float = 0.05
    similarity_threshold: float = 0.3
    similarity_steepness: float = 10.0
    candidate_mode: Literal["exact", "blocked"] = "exact"
    candidate_pool_multiplier: float = 12.0
    min_candidate_pool: int = 80
    blocking_attributes: list[str] = Field(default_factory=list)
    similarity_workers: int = 1
    similarity_chunk_size: int = 64
    checkpoint_every_rows: int = 250
    triadic_closure_prob: float = 0.6
    target_clustering: float = 0.35
    target_modularity: float = 0.55  # Target modularity (0.4-0.7 range)
    community_count: int | None = None  # Auto-detected if None
    inter_community_scale: float = (
        0.3  # Initial edge prob between communities vs within
    )
    max_calibration_iterations: int = 12  # Max iterations for adaptive calibration
    attribute_weights: dict[str, AttributeWeightConfig] = Field(default_factory=dict)
    degree_multipliers: list[DegreeMultiplierConfig] = Field(default_factory=list)
    edge_type_rules: list[EdgeTypeRule] = Field(default_factory=list)
    influence_factors: list[InfluenceFactorConfig] = Field(default_factory=list)
    default_edge_type: str = "peer"
    ordinal_levels: dict[str, dict[str, int]] = Field(default_factory=dict)
    generated_from: str | None = None
    generation_rationale: str | None = None
    seed: int | None = None

    def get_total_weight(self) -> float:
        """Get total weight for normalization."""
        return sum(cfg.weight for cfg in self.attribute_weights.values())

    def get_ordinal_levels(self, attribute: str) -> dict[str, int] | None:
        """Get ordinal levels for an attribute.

        Checks attribute-level config first, then falls back to
        config-level ordinal_levels.
        """
        attr_config = self.attribute_weights.get(attribute)
        if attr_config and attr_config.ordinal_levels is not None:
            return attr_config.ordinal_levels
        return self.ordinal_levels.get(attribute)

    def to_yaml(self, path: str | Path) -> None:
        """Save config to a YAML file."""
        path = Path(path)
        data = self.model_dump(exclude_none=True, exclude_defaults=False)
        # Clean up empty collections for readability
        for key in list(data.keys()):
            if isinstance(data[key], (list, dict)) and not data[key]:
                del data[key]
        path.parent.mkdir(parents=True, exist_ok=True)
        with open(path, "w") as f:
            yaml.dump(data, f, default_flow_style=False, sort_keys=False)

    @classmethod
    def from_yaml(cls, path: str | Path) -> "NetworkConfig":
        """Load config from a YAML file."""
        path = Path(path)
        with open(path) as f:
            data = yaml.safe_load(f) or {}
        return cls.model_validate(data)
